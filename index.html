<!DOCTYPE html><!--Author: Pranav Rajpurkar 2016--><html xmlns="http://www.w3.org/1999/html"><head>
    <meta charset="utf-8">
    <title> WikiDO </title>
<!--    <meta name="description" content="Yale Spider is a large dataset for complex and cross-domain semantic parsing and text-to-SQL Task introduced by our EMNLP 2018 paper. It was annotated by 11 Yale students. It can be used for developing natural language interfaces for relational databases">-->
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<!--    <link rel="shortcut icon" href="images/spider_logo.png" />-->
    <link rel="stylesheet" href="css/main.css">
    <link rel="canonical" href="#">
    <!--- <link rel="alternate" type="application/rss+xml" title="yale-lily.github.io" href="http://yale-lily.github.io//feed.xml"> --->
    <script type="text/javascript" src="js/MathJax.js"></script>
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="css/bootstrap.min.css" crossorigin="anonymous">
    <script src="js/jquery.min.js"></script>
    <script src="js/bib-list.js"></script>
    <!-- <link rel="stylesheet" href="/bib-publication-list.css" type="text/css" /> -->
    <link rel="stylesheet" href="css/spider.css" type="text/css">
    <!-- Latest compiled and minified JavaScript -->
    <script src="js/bootstrap.min.js" crossorigin="anonymous"></script>
</head>
<body>
<div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation">
    <div class="container clearfix">
        <!--<div class="rightNav">
            <div class="collapse navbar-collapse" id="navbar">
                <ul class="nav navbar-nav navbar-right">
                    <li><a>Home</a></li>
                </ul>
            </div>
        </div>-->
        <div class="leftNav">
            <div class="brandDiv">
<!--                <a href="#"><img src="images/lily-logo.png" alt="test image" height="40" /></a>-->
            </div>
        </div>
    </div>
</div>
<div class="cover" id="topCover">
    <div class="container" style="width:1370px">
        <div class="row">
            <div class="col-md-12">
                <h1 id="appTitle" height="40">
                    <img src="images/LOGO.drawio.png" height="110">
<!--                    <img src="images/spider_logo.png" alt="test image" height="100" />-->
                </h1>
            </div>
            <h2 id="appSubtitle">A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models </h2>
        </div>
    </div>
</div>
<div class="cover" id="contentCover">
    <div class="container">
        <div class="row">
            <div class="column">
                <div class="infoCard">
                    <div class="infoBody">
                        <div class="infoHeadline">
                            <h2>What is WikiDO?</h2>
                        </div>
                        <p align="left">
                        </p><div class="left">
                        To address this gap, we introduce WikiDO (drawn from <b>Wiki</b>pedia <b>D</b>diversity <b>O</b>bservatory), a new cross-modal retrieval benchmark to assess the OOD generalization capabilities of pretrained VLMs. This consists of 380K image-text pairs from Wikipedia with domain labels, along with carefully curated, human-verified in-distribution (ID) and OOD test sets of size 3K each. The image-text pairs are very diverse in topics.
                        </div>
                        <p></p>
                        <div class="infoHeadline">
                            <h2>Why WikiDO?</h2>
                        </div>
                        <p align="left">
                        </p><div class="left">
                        Cross-modal (image-to-text and text-to-image) retrieval is an established task used in evaluation benchmarks to test the performance of vision-language models (VLMs). Several state-of-the-art VLMs (e.g. CLIP, BLIP-2) have achieved near-perfect performance on widely-used image-text retrieval benchmarks such as MSCOCO-Test-5K and Flickr30K-Test-1K. As a measure of out-of-distribution (OOD) generalization, prior works rely on zero-shot performance evaluated on one dataset (Flickr) using a VLM finetuned on another one (MSCOCO). We argue that such comparisons are insufficient to assess the OOD generalization capability of models due to high visual and linguistic similarity between the evaluation and finetuning datasets. WikiDO offers a strong cross-modal retrieval benchmark for current VLMs, especially for evaluating OOD generalization.
                        <a class="btn actionBtn" href="https://arxiv.org/abs/2305.13040">WikiDO Paper</a>
                        </div>
                        <p></p>
                        <div class="infoHeadline">
                            <h2>Getting Started</h2>
                        </div>
                        <p align="left">
                        </p><div class="left"> The data is split into training, dev, and test sets. Download the dataset here (distributed under the <a href="https://creativecommons.org/licenses/by-nc/4.0/legalcode">CC BY-NC 4.0</a> license):
                        </div>
                        <p></p>
                        <a class="btn actionBtn inverseBtn" href="https://spokenwoz.oss-cn-wulanchabu.aliyuncs.com/audio_5700_train_dev.tar.gz">WikiDO all images</a>
                        <a class="btn actionBtn inverseBtn" href="https://spokenwoz.oss-cn-wulanchabu.aliyuncs.com/text_5700_train_dev.tar.gz">WikiDO all captions</a>
<!--                         <a class="btn actionBtn inverseBtn" href="https://spokenwoz.oss-cn-wulanchabu.aliyuncs.com/audio_5700_test.tar.gz">SpokenWOZ Audio Test Set</a>
                        <a class="btn actionBtn inverseBtn" href="https://spokenwoz.oss-cn-wulanchabu.aliyuncs.com/text_5700_test.tar.gz">SpokenWOZ Text Test Set</a> -->
                        <p align="left">
                        </p><div class="left"> Details of baseline models and evaluation script can be found on the following GitHub site:
                        <a class="btn actionBtn inverseBtn" href="https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/spokenwoz" download="">WikiDO Github Page</a>
                        </div>
                        <div class="left">We will update the models and results on the leaderboard based on the publicly available papers. Feel free to contact <a href="tankalapavankalyan@gmail.com">Pavan Kalyan</a> if you want to submit your results.
                        </div>
                        <p></p>
                        <div class="infoHeadline">
                            <h2>How we construct WikiDO?</h2>
                        </div>
                        <p align="left">
                        </p><div class="left"> WikiDO consists of image-text data derived from <em>Wikipedia Diversity Observatory</em>, a diverse source of Wikipedia articles spanning several diversity axes including geography, gender, ethnicity and domains/topics. We focus on the domains axis that is most diverse in terms of coverage and spans different topics (as determined via topic labels assigned to each article) such as food, books, fashion and sports. 
                        </div>
                        <p></p>
                        <img src="images/WIKIDO_splits.drawio.png">
                        <p></p>
                         <div class="infoHeadline">
                            <h2>Have Questions or Want to Contribute ?</h2>
                        </div>
                        <p align="left">
                        </p><div class="left">  Feel free to contact <a href="sishuzheng@foxmail.com">Pavan Kalyan</a> and <a href="mawentao.mwt@alibaba-inc.com">Piyush Pasi</a>. We would greatly appreciate it if you could provide us your helpful suggestions for this project.
                        </div>
                        <p></p>
<!--                          <div class="infoHeadline">
                            <h2>Acknowledgement</h2>
                        </div>
                        <p align="left">
                        </p><div class="left">  We would like to thank Prof. <a href="https://scholar.google.co.uk/citations?user=yDSBDJoAAAAJ&amp;hl=en">Milica Gasic</a> for her appreciation and advice on our idea at the beginning of the project. We also thank Dr. <a href="https://scholar.google.com/citations?user=oHoEp34AAAAJ&amp;hl=zh-CN">Bowen Yu</a> for his constructive comments on our writing. Finally, we would like to thank all the annotators for their efforts. We look forward to our dataset advancing the research on spoken TOD.
                        </div>
                        <p></p>
                        <div class="infoHeadline">
                            <h2>Citation</h2>
                        </div>
                        <p align="left">
                        </p><div class="left" style="background-color: #f5f5f5">  @inproceedings{NEURIPS2023_7b16688a,<br>
                                              author = {Si, Shuzheng and Ma, Wentao and Gao, Haoyu and Wu, Yuchuan and Lin, Ting-En and Dai, Yinpei and Li, Hangyu and Yan, Rui and Huang, Fei and Li, Yongbin},<br>
                                              booktitle = {Advances in Neural Information Processing Systems},<br>
                                              editor = {A. Oh and T. Neumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},<br>
                                              pages = {39088--39118},<br>
                                              publisher = {Curran Associates, Inc.},<br>
                                              title = {SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents},<br>
                                              url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/
                            7b16688a2b053a1b01474ab5c78ce662-Paper-Datasets_and_Benchmarks.pdf},<br>
                                              volume = {36},<br>
                                              year={2023}}
                        </div>
                        <p></p>
 -->
                        </div>
                    </div>
                </div>
            <div class="column">
                <div class="infoCard">
                    <div class="infoBody">
                        <div class="infoHeadline">
                            <h2>Leadboard</h2>
                        </div>
                        <p align="left">
                        </p><div class="left">
                        Here we will rank different VLMs based on their performance on OOD split of WikiDO.
                        </div>
                        <p></p>

                        <div class="infoHeadline">
                            <h4>Image-Text retrieval</h4>
                        </div>
                        <p align="left">
                        </p><div class="left">
                        We use Recall as a metric for evaluation. Mean of (R@1, R@5, R@10) is presented here. I2T expands to image-to-text retrieval while T2I expands to text-to-image retrieval.
                        </div>
                        <p></p>
                        <table class="table performanceTable">
                            <tbody><tr>
                                <th>Rank</th>
                                <th>Model</th>
                                <th>ID I2T</th>
                                <th>ID T2I</th>
                                <th>OOD I2T</th>
                                <th>OOD T2I</th>
                            </tr>
                            <tr>
                                <td>
                                    <p>1</p>
                                    <span class="date label label-default">June 1, 2023</span>
                                </td>
                               <td style="word-break:break-word;">
                                  CLIP (ViT-L 336)<sub>428 M</sub>
<!--                              <p class="institution">Alibaba DAMO</p> -->
                                  <a class="link" href="https://arxiv.org/abs/2103.00020">(Radford, Alec et al. 2021)</a>
                               </td>
                                <td>91.7</td>
                                <td>90.9</td>
                                <td>84.3</td>
                                <td>84.4</td>
                            </tr>
                            <tr>
                                <td>
                                    <p>2</p>
                                    <span class="date label label-default">June 1, 2023</span>
                                </td>
                               <td style="word-break:break-word;">
                                   BLIP 2 (ViT-L)<sub>473 M</sub>
<!--                               <p class="institution">Alibaba DAMO</p> -->
                                   <a class="link" href="https://arxiv.org/abs/2301.12597">(Li, Junnan et al. 2023)</a>
                               </td>
                                <td>90.9</td>
                                <td>91.2</td>
                                <td>82.8</td>
                                <td>83.7</td>
                            </tr>
                            <tr>
                                <td>
                                    <p>3</p>
                                    <span class="date label label-default">June 1, 2023</span>
                                </td>
                               <td style="word-break:break-word;">
                                  BLIP 2 (ViT-G)<sub>1172 M</sub>
<!--                               <p class="institution">Alibaba DAMO</p> -->
                                   <a class="link" href="https://arxiv.org/abs/2301.12597">(Li, Junnan et al. 2023)</a>
                               </td>
                                <td>89.6</td>
                                <td>89.8</td>
                                <td>81.0</td>
                                <td>82.4</td>
                            </tr>
                            <tr>
                                <td>
                                    <p>4</p>
                                    <span class="date label label-default">June 1, 2023</span>
                                </td>
                               <td style="word-break:break-word;">
                                  BLIP (ViT-L)<sub>446 M</sub>
<!--                               <p class="institution">Alibaba DAMO</p> -->
                                   <a class="link" href="https://arxiv.org/abs/2201.12086">(Li, Junnan et al. 2022)</a>
                               </td>
                                <td>86.0</td>
                                <td>86.1</td>
                                <td>76.9</td>
                                <td>78.2</td>
                            </tr>
                            <tr>
                                <td>
                                    <p>5</p>
                                    <span class="date label label-default">June 1, 2023</span>
                                </td>
                               <td style="word-break:break-word;">
                                  BLIP (ViT-B)<sub>223 M</sub>
<!--                               <p class="institution">Alibaba DAMO</p> -->
                                   <a class="link" href="https://arxiv.org/abs/2201.12086">(Li, Junnan et al. 2022)</a>
                               </td>
                                <td>86.2</td>
                                <td>85.6</td>
                                <td>75.4</td>
                                <td>76.0</td>
                            </tr>

                        </tbody></table>

                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<!--<footer class="site-footer">

    <div class="wrapper">

        <h2 class="footer-heading">yale-lily.github.io</h2>

        <div class="footer-col-wrapper">
            <div class="footer-col footer-col-1">
                <ul class="contact-list">
                    <li>yale-lily.github.io</li>
                    <li><a href="mailto:dragomir.radev@yale.edu">dragomir.radev@yale.edu</a></li>
                </ul>
            </div>

            <div class="footer-col footer-col-2">
                <ul class="social-media-list">

                    <li>
                        <a href="https://github.com/Yale-LILY"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">Yale-LILY</span></a>

                    </li>



                </ul>
            </div>

            <div class="footer-col footer-col-3">
                <p>Website for the LILY Group at Yale University
                </p>
            </div>
        </div>

    </div>

</footer>-->



</body></html>
